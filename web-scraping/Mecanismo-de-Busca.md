## Criar um crawler

Independente do mecanismo de busca que você for criar (seja ele vertical ou genérico), você terá de criar um crawler primeiro. Um crawler ou spider é um “robô” (um algoritmo que funciona sozinho) que busca informações em uma determinada fonte de dados e cataloga elas no índice do seu mecanismo de busca, para que as consultas posteriores passem a considerar aqueles novos dados em seus resultados. Sem o trabalho do crawler é impossível criar um mecanismo de busca eficiente.

Em um buscador vertical, por exemplo de carros, o crawler acessa uma lista de sites de classificados de carros e procura as informações de cada um dos anúncios, coletando elas de maneira organizada e salvando em um banco de dados. Em um buscador de informações, o crawler acessa uma lista de notícias ou documentos e salva as partes que interessam para a busca em um banco de dados. Dependendo das suas necessidades o seu crawler pode separar a informações em categorias (carros, motos e caminhões, por exemplo), ou então as partes dela conforme seu conteúdo (imagens, notícias e arquivos, por exemplo)

É importante frisar que seu crawler deve ser um algoritmo separado do seu buscador, que deverá ficar rodando periodicamente para manter sua base de dados atualizada. Dependendo do volume de informações que você precisa atualizar na base, talvez seja interessante fazê-lo durante os horários de menor audiência para evitar lentidões em seu buscador. E dependendo da periodicidade com que os dados são atualizados, talvez você precise criar uma política para não ter de atualizar toda sua base toda vez que o crawler rodar.

Este processo de funcionamento do crawler chamamos de webcrawling. Webcrawlers como os do Google (chamado de GoogleBot) são muito complexos e não apenas lêem uma lista de sites mas “seguem” cada um dos links de cada site percorrendo assim todo emaranhado que é a nossa web atual. Cada vez que ele encontra um novo site que ele não conhecia, coloca-o no índice principal do Google para passar a ser rastreado periodicamente a partir de então.

## Criar um índice

Quando o seu crawler ler as informações de uma página ou documento você vai querer salvar apenas aquelas que são importantes para a busca, evitando que seu banco de dados vire algo colossal. As informações que lhe serão úteis na pesquisa variam de vertical para vertical, mas geralmente são um reflexo dos tipos de filtros que você irá disponibilizar ao seu usuário no site de busca, sendo um campo de texto livre o filtro mais comum (como o Google faz). Esse compilado de informações úteis para a pesquisa é o que chamamos de índice e é aqui onde mora a real complexidade de se criar um mecanismo que funcione de maneira eficaz e eficiente.

Os índices mais comuns de serem criados para mecanismos de busca baseados em texto são os índices invertidos. Por padrão os bancos de dados criam o que chamamos de forward index, ou índices diretos, onde com base no valor de uma chave do registro/documento, encontramos o restante do seu conteúdo. A ideia do índice invertido é o oposto: com base em um pedaço do conteúdo do registro/documento, encontramos sua chave para trazer o registro completo. Afinal, o usuário sabe algumas palavras do que ele vai buscar, mas não sabe qual a chave do registro/documento que ele quer, certo?!